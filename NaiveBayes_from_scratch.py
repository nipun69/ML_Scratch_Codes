# -*- coding: utf-8 -*-
"""Asgn_4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JifanhieoTF8DaTUjTZJCg4ggRQ7qxL_
"""

# Name-Nipun Bharadwaj
# Roll no.- 22AG30027
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Implementation
class NaiveBayes:
    def __init__(self, num_bins=3):
        self.num_bins = num_bins
        self.bins = None
        self.class_probabilities = None
        self.attribute_probabilities = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.classes = np.unique(y)
        n_classes = len(self.classes)

        # Initialize arrays for storing probabilities
        self.class_probabilities = np.zeros(n_classes)
        self.attribute_probabilities = np.zeros((n_classes, n_features, self.num_bins))

        # Divide each attribute value into K bins
        self._create_bins(X)

        # Compute class probabilities
        for idx, c in enumerate(self.classes):
            X_c = X[y == c]
            self.class_probabilities[idx] = len(X_c) / float(n_samples)

            # Compute attribute probabilities for each class
            for i in range(n_features):
                for j in range(self.num_bins):
                    bin_indices = np.where((self.bins[i][j] <= X_c[:, i]) & (X_c[:, i] < self.bins[i][j+1]))
                    self.attribute_probabilities[idx, i, j] = len(bin_indices[0]) / float(len(X_c))

    def _create_bins(self, X):
        self.bins = []
        n_features = X.shape[1]

        for i in range(n_features):
            feature_bins = np.linspace(np.min(X[:, i]), np.max(X[:, i]), self.num_bins + 1)
            self.bins.append(feature_bins)

    def predict(self, X):
        predictions = [self._predict_sample(x) for x in X]
        return np.array(predictions)

    def _predict_sample(self, x):
        probabilities = []

        for idx, c in enumerate(self.classes):
            class_probability = np.log(self.class_probabilities[idx])
            attribute_probability = 0

            for i, feature_value in enumerate(x):
                for j, bin_value in enumerate(self.bins[i]):
                    if j < self.num_bins - 1:
                        if self.bins[i][j] <= feature_value < self.bins[i][j + 1]:
                            attribute_probability += np.log(self.attribute_probabilities[idx, i, j])
                            break

            probabilities.append(class_probability + attribute_probability)

        return self.classes[np.argmax(probabilities)]

def accuracy(y_true, y_pred):
    correct = np.sum(y_true == y_pred)
    total = len(y_true)
    return correct / total * 100

df=pd.read_csv("Iris.csv")
df.drop('Id',axis=1,inplace=True)
df

mapp={'Iris-setosa':1.0,'Iris-versicolor':2.0,'Iris-virginica':3.0}
y=df['Species'].map(mapp).values.astype(float)
y

X=df.drop(columns=['Species']).dropna().values.astype(float)
X

# Experiment 1
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)
k_vals=[2,3,5]
Accuracy=[]
for i in k_vals:
  model=NaiveBayes(num_bins=i)
  model.fit(X_train,y_train)
  y_preds=model.predict(X_test)
  acc=accuracy(y_test,y_preds)
  Accuracy.append(acc)
print(Accuracy)
best_k=k_vals[np.argmax(Accuracy)]
print("best hyperparameter is k=",best_k)

plt.figure(figsize=(10, 6))
plt.plot(k_vals, Accuracy, marker='o', linestyle='-', color='b')
plt.title('Accuracy vs Value of hyperparameter "k" ')
plt.xlabel('Hyperparameter (k)')
plt.ylabel('Accuracy')
plt.xticks(k_vals)
plt.grid(True)
plt.show()

# Experiment 2
def add_noise(data, fraction=0.1):
    if data.ndim == 1:
        data = data.reshape(-1, 1)
    number_of_samples = int(len(data) * fraction)
    noise_indices = np.random.choice(len(data), number_of_samples, replace=False)
    noise = np.random.normal(loc=np.mean(data[:, :-1]), scale=np.std(data[:, :-1]), size=(number_of_samples, data.shape[1] - 1))
    data[noise_indices, :-1] += noise
    return data

X_train.shape

type(y_train)
print(y_train.shape)

fractions = [0.1, 0.4, 0.8, 0.9]
acc_arr=[]
for i in fractions:
    X_train_noisy = add_noise(X_train.copy(), fraction=i)
    y_train_noisy = add_noise(y_train.copy(), fraction=i)
    y_train_noisy = y_train_noisy.ravel()
    model2 = NaiveBayes(num_bins=best_k)
    model2.fit(X_train_noisy, y_train_noisy)
    y_preds2 = model2.predict(X_test)
    Accur = accuracy(y_preds2, y_test)
    print("Model accuracy is:", Accur)
    acc_arr.append(Accur)

plt.figure(figsize=(10, 6))
plt.plot(fractions, acc_arr, marker='o', linestyle='-', color='b')
plt.title('Accuracy vs Fraction of noise ')
plt.xlabel('Fraction noise')
plt.ylabel('Accuracy')
plt.xticks(fractions)
plt.grid(True)
plt.show()

