# -*- coding: utf-8 -*-
"""assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pFpPN2ISL4EeHk8k2d_CqNMxIOADjsA7
"""
# Name-Nipun Bharadwaj
# Roll Number-22AG30027
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Function for z score normalisation
def z_score_norm(df):

  # Calculate the mean and standard deviation of each column.
  mu = df.mean()
  sigma = df.std()

  # Normalize each column.
  for column in df.columns:
    df[column] = (df[column] - mu[column]) / sigma[column]

  return df

df=pd.read_csv("Iris.csv")
df

df2 = z_score_norm(df.drop(columns=['Species', 'Id']))
df2

X = df2
X = X.values.astype(float)
X

species_map = {"Iris-setosa": 1.0, "Iris-versicolor": 2.0, "Iris-virginica": 3.0}
y = df["Species"].map(species_map)
y = y.values.astype(float)
# y1=z_score_norm(y)
y

# Data set splitting
df = df.sample(frac=1)

# Select the ratio to split the data frame into test and train sets
test_size = 0.3

# Split data frames into training and testing data frames using slicing
X_train = X[:int(len(df2) * (1 - test_size))]
X_test = X[int(len(df2) * (1 - test_size)):]
y_train = y[:int(len(df) * (1 - test_size))]
y_test = y[int(len(df) * (1 - test_size)):]
# Print the shape of the training and testing data frames
print("Shape of the training data frame:", X_train.shape)
print("Shape of the testing data frame:", X_test.shape)

X_train

X_test

y_train

y_test

def accuracy_metric(actual, predicted):
    correct = 0
    for i in range(len(actual)):
        if actual[i] == predicted[i]:
            correct += 1
    return correct / float(len(actual)) * 100.0

#KNN Normal
class knn_normal:
    def __init__(self, k):
        self.k = k
        self.point = None

    def fit(self, X_train, y_train):
        self.X_train = X_train
        self.y_train = y_train

    def euclidean_distance(self, x1, x2):
        return np.sqrt(np.sum((x1 - x2) ** 2))

    def predict(self, X_test):
        predictions = [self._predict(X) for X in X_test]
        return np.array(predictions)

    def _predict(self, X):
        # Calculate distances between X and all examples in the training set
        distances = [self.euclidean_distance(X, X_train) for X_train in self.X_train]

        # Get indices of k-nearest training data points
        k_neighbors_indices = np.argsort(distances)[:self.k]

        # Get the labels of the k-nearest training data points
        k_neighbor_labels = [self.y_train[i] for i in k_neighbors_indices]

        # Return the most common class label among the k neighbors
        most_common = np.bincount(k_neighbor_labels).argmax()
        return most_common

# Knn Weighted
class knn_weigh:
    def __init__(self, k):
        self.k = k

    def fit(self, X_train, y_train):
        self.X_train = X_train
        self.y_train = y_train

    def euclidean_distance(self, x1, x2):
        return np.sqrt(np.sum((x1 - x2) ** 2))

    def predict(self, X_test):
        predictions = [self._predict(x) for x in X_test]
        return np.array(predictions)

    def _predict(self, x):
        # Calculate distances between x and all examples in the training set
        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train]

        # Get indices of k-nearest training data points
        k_neighbors_indices = np.argsort(distances)[:self.k]

        # Get the labels and distances of the k-nearest training data points
        k_neighbor_labels = [self.y_train[i] for i in k_neighbors_indices]
        k_neighbor_distances = [distances[i] for i in k_neighbors_indices]

        # Calculate weights based on distances
        weights = 1 / (np.array(k_neighbor_distances) + 1e-6)  # Adding a small value to avoid division by zero

        # Weighted sum of class labels
        weighted_sum = np.sum(weights * k_neighbor_labels)

        # Normalize the weighted sum
        prediction = weighted_sum / np.sum(weights)

        return int(round(prediction))

model=knn_normal(k=3)
model.fit(X_train,y_train)
predictions = model.predict(X_test)
predictions

# Iterating for best k normal
k_values = [1, 3, 5, 10, 20]
accuracy_values = []


for k in k_values:

    knn = knn_normal(k=k)
    knn.fit(X_train, y_train)


    y_pred = knn.predict(X_test)


    accuracy = accuracy_metric(y_test, y_pred)
    accuracy_values.append(accuracy)

print(accuracy_values)
best_k=k_values[np.argmax(accuracy_values)]
print(best_k)

plt.figure(figsize=(10, 6))
plt.plot(k_values, accuracy_values, marker='o', linestyle='-', color='b')
plt.title('Accuracy vs. Number of Neighbors (k) for KNN on Iris Dataset')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy')
plt.xticks(k_values)
plt.grid(True)
plt.show()

from sklearn.metrics import confusion_matrix
best_knn_normal = knn_normal(k=best_k)
best_knn_normal.fit(X_train, y_train)

# Make predictions on the test data using the best K
y_pred_best = best_knn_normal.predict(X_test)

conf_matrix = confusion_matrix(y_test, y_pred_best)
print(f"Confusion Matrix for K={best_k}:\n", conf_matrix)
conf_matrix = confusion_matrix(y_test, y_pred_best)
plt.figure(figsize=(6, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds')
plt.title(f'Confusion Matrix for K={best_k}')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Iterating for best k in weighted
k_values = [1, 3, 5, 10, 20]
accuracy_values = []


for k in k_values:

    knn = knn_weigh(k=k)
    knn.fit(X_train, y_train)


    y_pred = knn.predict(X_test)


    accuracy = accuracy_metric(y_test, y_pred)
    accuracy_values.append(accuracy)

print(accuracy_values)

best_k=k_values[np.argmax(accuracy_values)]
print(best_k)

plt.figure(figsize=(10, 6))
plt.plot(k_values, accuracy_values, marker='o', linestyle='-', color='b')
plt.title('Accuracy vs. Number of Neighbors (k) for KNN on Iris Dataset')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy')
plt.xticks(k_values)
plt.grid(True)
plt.show()

best_knn_weigh = knn_weigh(k=best_k)
best_knn_weigh.fit(X_train, y_train)

# Make predictions on the test data using the best K
y_pred_best = best_knn_weigh.predict(X_test)

conf_matrix = confusion_matrix(y_test, y_pred_best)
print(f"Confusion Matrix for K={best_k}:\n", conf_matrix)
conf_matrix = confusion_matrix(y_test, y_pred_best)
plt.figure(figsize=(6, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds')
plt.title(f'Confusion Matrix for K={best_k}')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# adding noise to dataset and testing
def add_noise(data, noise_fraction=0.1):
    num_samples_to_noise = int(len(data) * noise_fraction)
    noise_indices = np.random.choice(len(data), num_samples_to_noise, replace=False)
    noise = np.random.normal(loc=0, scale=1.0, size=(num_samples_to_noise, data.shape[1]-1))
    data[noise_indices, :-1] += noise
    return data

X_train_noisy = add_noise(X_train.copy(), noise_fraction=0.1)

X_train_noisy

knn_noisy_normal=knn_normal(k=best_k)
knn_noisy_normal.fit(X_train_noisy,y_train)
y_pred_noisy=knn_noisy_normal.predict(X_test)
acc=accuracy_metric(y_pred_noisy,y_test)
print(acc)

knn_noisy_weigh=knn_weigh(k=best_k)
knn_noisy_weigh.fit(X_train_noisy,y_train)
y_pred_noisy_w=knn_noisy_weigh.predict(X_test)
acc1=accuracy_metric(y_pred_noisy_w,y_test)
print(acc1)

# Petal parameters
X_petal = df2.drop(columns=['SepalLengthCm', 'SepalWidthCm'])
X_petal = X_petal.values.astype(float)
X_petal

test_size = 0.3

# Split data frames into training and testing data frames using slicing
X_train_petal = X_petal[:int(len(X_petal) * (1 - test_size))]
X_test_petal = X_petal[int(len(X_petal) * (1 - test_size)):]

# Print the shape of the training and testing data frames
print("Shape of the training data frame:", X_train_petal.shape)
print("Shape of the testing data frame:", X_test_petal.shape)

knn_petal=knn_normal(k=best_k)
knn_petal.fit(X_train_petal,y_train)
y_pred_p=knn_petal.predict(X_test_petal)
accur=accuracy_metric(y_pred_p,y_test)
print(accur)

# Sepal parameters
X_sepal = df2.drop(columns=['PetalLengthCm', 'PetalWidthCm'])
X_sepal = X_sepal.values.astype(float)
X_sepal

test_size = 0.3

# Split data frames into training and testing data frames using slicing
X_train_sepal = X_sepal[:int(len(X_sepal) * (1 - test_size))]
X_test_sepal = X_sepal[int(len(X_sepal) * (1 - test_size)):]

# Print the shape of the training and testing data frames
print("Shape of the training data frame:", X_train_sepal.shape)
print("Shape of the testing data frame:", X_test_sepal.shape)

knn_sepal=knn_normal(k=best_k)
knn_sepal.fit(X_train_sepal,y_train)
y_pred_s=knn_sepal.predict(X_test_sepal)
accur1=accuracy_metric(y_pred_s,y_test)
print(accur1)

# Length Parameters
X_l = df2.drop(columns=['SepalWidthCm', 'PetalWidthCm'])
X_l = X_l.values.astype(float)
X_l

test_size = 0.3

# Split data frames into training and testing data frames using slicing
X_train_l = X_l[:int(len(X_l) * (1 - test_size))]
X_test_l = X_l[int(len(X_l) * (1 - test_size)):]

# Print the shape of the training and testing data frames
print("Shape of the training data frame:", X_train_l.shape)
print("Shape of the testing data frame:", X_test_l.shape)

knn_l=knn_normal(k=best_k)
knn_l.fit(X_train_l,y_train)
y_pred_l=knn_l.predict(X_test_l)
accur2=accuracy_metric(y_pred_l,y_test)
print(accur2)

# Width Parameters
X_w = df2.drop(columns=['SepalLengthCm', 'PetalLengthCm'])
X_w = X_w.values.astype(float)
X_w

test_size = 0.3

# Split data frames into training and testing data frames using slicing
X_train_w = X_w[:int(len(X_w) * (1 - test_size))]
X_test_w= X_w[int(len(X_w) * (1 - test_size)):]

# Print the shape of the training and testing data frames
print("Shape of the training data frame:", X_train_w.shape)
print("Shape of the testing data frame:", X_test_w.shape)

knn_w=knn_normal(k=best_k)
knn_w.fit(X_train_w,y_train)
y_pred_w=knn_w.predict(X_test_w)
accur3=accuracy_metric(y_pred_w,y_test)
print(accur3)

